# ðŸš€ Olist E-commerce Data Pipeline

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Dagster](https://img.shields.io/badge/Dagster-1.6+-orange.svg)](https://dagster.io/)
[![DuckDB](https://img.shields.io/badge/DuckDB-0.10+-yellow.svg)](https://duckdb.org/)
[![dbt](https://img.shields.io/badge/dbt-1.7+-red.svg)](https://www.getdbt.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> Production-grade batch data engineering pipeline for processing Brazilian e-commerce data using modern data stack

## ðŸŽ¯ Overview

This project implements a **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) data pipeline for processing e-commerce transaction data from [Olist Brazilian E-Commerce Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce). 

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Sources                            â”‚
â”‚          CSV Files (Customers, Orders, Items)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Bronze Layer (Raw Data)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  PostgreSQL  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  MinIO/S3      â”‚              â”‚
â”‚  â”‚  (Staging)   â”‚         â”‚  (Parquet)     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Silver Layer (Cleaned Data)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚         DuckDB Warehouse             â”‚                  â”‚
â”‚  â”‚  â€¢ raw schema (loaded from Parquet)  â”‚                  â”‚
â”‚  â”‚  â€¢ staging schema (dbt views)        â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Gold Layer (Analytics)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚       DuckDB Analytics Marts         â”‚                  â”‚
â”‚  â”‚  â€¢ fct_orders (fact table)           â”‚                  â”‚
â”‚  â”‚  â€¢ dim_customers (dimension)         â”‚                  â”‚
â”‚  â”‚  â€¢ daily_sales_summary               â”‚                  â”‚
â”‚  â”‚  â€¢ top_customers                     â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | [Dagster 1.6+](https://dagster.io/) | Workflow orchestration and monitoring |
| **Data Processing** | [Polars 0.20+](https://www.pola.rs/) | Fast DataFrame operations |
| **Analytics DB** | [DuckDB 0.10+](https://duckdb.org/) | OLAP database for analytics |
| **Transformation** | [dbt 1.7+](https://www.getdbt.com/) | SQL-based data transformations |
| **Object Storage** | [MinIO](https://min.io/) | S3-compatible object storage |
| **Staging DB** | [PostgreSQL 15](https://www.postgresql.org/) | Relational database for raw data |
| **Containerization** | [Docker](https://www.docker.com/) | Container orchestration |
| **File Format** | [Apache Parquet](https://parquet.apache.org/) | Columnar storage format |

## ðŸš€ Quick Start

### Prerequisites

- **Docker**
- **Python 3.11+**
- **8GB+ RAM**
- **Git**

### Installation

1ï¸âƒ£ **Clone the repository**

```bash
git clone https://github.com/yourusername/de-batch-pipeline.git
cd de-batch-pipeline
```

2ï¸âƒ£ **Create environment file**

```bash
cat > .env << 'EOF'
# PostgreSQL
POSTGRES_USER=dagster
POSTGRES_PASSWORD=dagster_password
POSTGRES_DB=dagster

# MinIO
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# DuckDB
DUCKDB_THREADS=2
DUCKDB_MEMORY_LIMIT=1GB

# Polars
POLARS_MAX_THREADS=2
EOF
```

3ï¸âƒ£ **Add your data files**

Place your CSV files in `data/seeds/`:
- `olist_customers_dataset.csv`
- `olist_orders_dataset.csv`
- `olist_order_items_dataset.csv`

4ï¸âƒ£ **Start the pipeline**

```bash
# Using Docker
docker compose up -d --build
```

5ï¸âƒ£ **Access Dagster UI**

Open [http://localhost:3000](http://localhost:3000)

6ï¸âƒ£ **Materialize assets**

In Dagster UI:
- Navigate to **Assets**
- Click **Materialize all**
- Monitor execution in real-time

## ðŸ“ Project Structure

```
de-batch-pipeline/
â”œâ”€â”€ ðŸ“‚ orchestration/              # Dagster assets and resources
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ ingest_raw_data.py     # CSV â†’ PostgreSQL
â”‚   â”‚   â”œâ”€â”€ export_to_minio.py     # PostgreSQL â†’ MinIO
â”‚   â”‚   â”œâ”€â”€ load_to_duckdb.py      # MinIO â†’ DuckDB
â”‚   â”‚   â””â”€â”€ dbt_transformations.py # dbt model execution
â”‚   â””â”€â”€ resources.py               # Database connections
â”‚
â”œâ”€â”€ ðŸ“‚ transformation/              # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/               # Silver layer (cleaned data)
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_order_items.sql
â”‚   â”‚   â””â”€â”€ marts/                 # Gold layer (analytics)
â”‚   â”‚       â”œâ”€â”€ core/
â”‚   â”‚       â”‚   â”œâ”€â”€ fct_orders.sql
â”‚   â”‚       â”‚   â””â”€â”€ dim_customers.sql
â”‚   â”‚       â””â”€â”€ analytics/
â”‚   â”‚           â”œâ”€â”€ daily_sales_summary.sql
â”‚   â”‚           â””â”€â”€ top_customers.sql
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ ðŸ“‚ data/
â”‚   â”œâ”€â”€ seeds/                     # Input CSV files
â”‚   â””â”€â”€ warehouse.duckdb           # DuckDB database (generated)
â”‚
â”œâ”€â”€ ðŸ“‚ dagster_home/
â”‚   â””â”€â”€ dagster.yaml               # Dagster configuration
â”‚
â”œâ”€â”€ ðŸ“‚ docker/
â”‚   â””â”€â”€ dagster/
â”‚       â””â”€â”€ Dockerfile             # Dagster container
â”‚
â”œâ”€â”€ docker-compose.yaml            # Service orchestration
â”œâ”€â”€ main.py                        # Dagster definitions
â”œâ”€â”€ requirements.txt               # Python dependencies
â””â”€â”€ README.md                      # This file
```

## ðŸ”„ Data Flow

### Pipeline Stages

#### 1ï¸âƒ£ **Ingestion** (CSV â†’ PostgreSQL)
```python
Assets: raw_customers_to_postgres, raw_orders_to_postgres, raw_order_items_to_postgres
```
- Reads CSV files with Polars
- Bulk loads into PostgreSQL
- Provides transactional safety

#### 2ï¸âƒ£ **Export** (PostgreSQL â†’ MinIO)
```python
Assets: export_customers_to_minio, export_orders_to_minio, export_order_items_to_minio
```
- Extracts data from PostgreSQL
- Converts to Parquet format (10-100x compression)
- Stores in MinIO S3-compatible storage

#### 3ï¸âƒ£ **Warehouse Load** (MinIO â†’ DuckDB)
```python
Asset: load_raw_data_to_duckdb
```
- Reads Parquet files from MinIO via S3 protocol
- Creates tables in DuckDB `raw` schema
- Enables high-performance analytics

#### 4ï¸âƒ£ **Transformation** (dbt)
```python
Assets: run_dbt_staging_models, run_dbt_marts_models
```
- **Staging**: Data cleaning and standardization
- **Marts**: Business-logic transformations and aggregations

### Asset Dependency Graph

```
raw_customers_to_postgres â”€â”€â”€â”€â”€â”
                                â”œâ”€â”€â†’ export_customers_to_minio â”€â”
                                â”‚                                â”‚
raw_orders_to_postgres â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                â”‚
                                â”œâ”€â”€â†’ export_orders_to_minio â”€â”€â”€â”€â”¼â”€â”€â†’ load_raw_data_to_duckdb
                                â”‚                                â”‚         â†“
raw_order_items_to_postgres â”€â”€â”€â”¤                                â”‚    run_dbt_staging_models
                                â””â”€â”€â†’ export_order_items_to_minioâ”˜         â†“
                                                                     run_dbt_marts_models
```

## ðŸ’» Usage

### Running the Full Pipeline

**Via Dagster UI:**
1. Open [http://localhost:3000](http://localhost:3000)
2. Navigate to **Assets** â†’ **View all assets**
3. Click **Materialize all**
4. Monitor progress in real-time

**Via CLI:**
```bash
# Materialize all assets
docker exec de_pipeline_dagster_web dagster asset materialize --select "*"

# Materialize specific asset group
docker exec de_pipeline_dagster_web dagster asset materialize --select "tag:group=ingestion"
```

### Querying Analytics

**Connect to DuckDB:**
```bash
docker exec -it de_pipeline_dagster_web python
```

```python
import duckdb

conn = duckdb.connect('/opt/dagster/app/data/warehouse.duckdb')

# Daily sales summary
conn.execute("""
    SELECT * FROM marts.daily_sales_summary 
    ORDER BY order_purchase_date DESC 
    LIMIT 10
""").df()

# Top customers
conn.execute("""
    SELECT * FROM marts.top_customers 
    LIMIT 20
""").df()
```

### Accessing Data

**MinIO Console:**
- URL: [http://localhost:9001](http://localhost:9001)
- Username: `minioadmin`
- Password: `minioadmin`

**PostgreSQL:**
```bash
docker exec -it de_pipeline_postgres psql -U dagster -d postgres

# List tables
\dt

# Query data
SELECT COUNT(*) FROM olist_orders_dataset;
```

## ðŸ”§ Troubleshooting

### Issue: Assets won't materialize

**Solution:**
```bash
# Check Dagster logs
docker logs de_pipeline_dagster_web --tail 100

# Reload definitions
# In Dagster UI: Click "Reload definitions"

# Restart Dagster
docker restart de_pipeline_dagster_web de_pipeline_dagster_daemon
```

### Issue: PostgreSQL connection errors

**Solution:**
```bash
# Verify PostgreSQL is healthy
docker exec de_pipeline_postgres pg_isready -U dagster

# Check logs
docker logs de_pipeline_postgres

# Restart if needed
docker restart de_pipeline_postgres
```

### Issue: DuckDB file locked

**Solution:**
```bash
# Stop all containers
docker compose down

# Remove DuckDB file
rm data/warehouse.duckdb*

# Restart
docker compose up -d
```

### Issue: Port already in use

**Solution:**
```bash
# Check what's using port 3000
lsof -i :3000

# Stop the process or change port in docker-compose.yaml
```

## ðŸ‘¨â€ðŸ’» Development

### Local Setup

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run tests (if available)
pytest
```

### Adding New Assets

1. Create asset in `orchestration/assets/`
2. Import in `main.py`
3. Add to `Definitions` object
4. Reload definitions in Dagster UI

### Modifying dbt Models

```bash
# Enter Dagster container
docker exec -it de_pipeline_dagster_web bash

# Run specific dbt model
cd transformation
dbt run --select model_name

# Run tests
dbt test
```

## ðŸ¤ Contributing

The project was created for educational purposes and inspired by modern data engineering best practices. Contributions are welcome!

## ðŸ“„ License

[LICENSE](LICENSE)
